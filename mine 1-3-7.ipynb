{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtNmnguuroyTLiuwjAcVEz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1 -  Prepare/Pre-process a text corpus to make it more usable for NLP tasks using\n","tokenization, filtration of stop words, removal of punctuation, stemming and\n","lemmatization."],"metadata":{"id":"yfBHgdJQFR34"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BE73B_uR5r4p"},"outputs":[],"source":["# pip install nltk\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","# 1. Download necessary NLTK data (run this once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","\n","def preprocess_text_nltk(text):\n","    print(f\"Original Text: \\n{text}\\n\")\n","\n","    # --- Step 1: Tokenization ---\n","    # Breaking the text into individual words/tokens\n","    tokens = word_tokenize(text)\n","\n","    # --- Step 2: Noise Removal (Stop words & Punctuation) ---\n","    # Load stop words (e.g., \"the\", \"is\", \"in\")\n","    stop_words = set(stopwords.words('english'))\n","    punctuation = set(string.punctuation)\n","\n","    filtered_tokens = []\n","    for token in tokens:\n","        # Convert to lower case for consistency\n","        word = token.lower()\n","\n","        # Filter out punctuation and stop words\n","        if word not in stop_words and word not in punctuation:\n","            filtered_tokens.append(word)\n","\n","    # --- Step 3: Stemming ---\n","    # Reducing words to their root form by chopping off ends (e.g., \"running\" -> \"run\")\n","    ps = PorterStemmer()\n","    stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n","\n","    # --- Step 4: Lemmatization ---\n","    # Reducing words to their dictionary root using vocabulary analysis (e.g., \"better\" -> \"good\")\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n","\n","    return stemmed_tokens, lemmatized_tokens\n","\n","# Example Usage\n","sample_text = \"The striped bats are hanging on their feet for best results!\"\n","stems, lemmas = preprocess_text_nltk(sample_text)\n","\n","print(f\"Stemmed:    {stems}\")\n","print(f\"Lemmatized: {lemmas}\")"]},{"cell_type":"markdown","source":["Q3 -  Extract the usernames from the email addresses present in a given text. ."],"metadata":{"id":"bhEOdS7xFdd_"}},{"cell_type":"code","source":["import re\n","\n","def extract_usernames(text):\n","    # Regex Breakdown:\n","    # ([a-zA-Z0-9._%+-]+)  -> Group 1: Match alphanumeric, dots, underscores, %, +, - (The Username)\n","    # @                    -> Match the literal '@' symbol\n","    # [a-zA-Z0-9.-]+       -> Match the domain name\n","    # \\.                   -> Match a literal dot\n","    # [a-zA-Z]{2,}         -> Match the extension (2+ letters like .com, .in, .org)\n","\n","    email_pattern = r'([a-zA-Z0-9._%+-]+)@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n","\n","    # re.findall returns a list of specific groups matched (in this case, just the username)\n","    usernames = re.findall(email_pattern, text)\n","\n","    return usernames\n","\n","# Test Data\n","corpus = \"\"\"\n","    Please contact support at help.desk@company.com for assistance.\n","    You can also reach the admin: admin_01@internal.net.\n","    Ignore invalid emails like bob@com or plain text.\n","    cc: jane.doe+test@gmail.co.uk\n","\"\"\"\n","\n","result = extract_usernames(corpus)\n","\n","print(f\"Extracted Usernames: {result}\")"],"metadata":{"id":"8sukdwbHEm17"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q7 - Extract all bigrams , trigrams using ngrams of nltk library"],"metadata":{"id":"kBe4FE5dFjEB"}},{"cell_type":"code","source":["import nltk\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","\n","# Ensure the tokenizer is downloaded\n","nltk.download('punkt')\n","\n","def extract_ngrams(text):\n","    # 1. Tokenize the text first\n","    # (N-grams work on a list of items, so we must split the string into words)\n","    tokens = word_tokenize(text)\n","\n","    # 2. Extract Bigrams (N=2)\n","    # The ngrams function returns a generator, so we convert it to a list to view it\n","    bigrams = list(ngrams(tokens, 2))\n","\n","    # 3. Extract Trigrams (N=3)\n","    trigrams = list(ngrams(tokens, 3))\n","\n","    return bigrams, trigrams\n","\n","# Example Data\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","bigrams_list, trigrams_list = extract_ngrams(text)\n","\n","print(f\"--- Original Text ---\\n{text}\\n\")\n","\n","print(f\"--- Bigrams (Total: {len(bigrams_list)}) ---\")\n","for bg in bigrams_list:\n","    print(bg)\n","\n","print(f\"\\n--- Trigrams (Total: {len(trigrams_list)}) ---\")\n","for tg in trigrams_list:\n","    print(tg)"],"metadata":{"id":"L-VWxu3FEsvt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q10 - Classify movie reviews as positive or negative from the IMDB movie dataset of\n","50K movie reviews. (Link for dataset:\n","https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-revie\n","ws )"],"metadata":{"id":"Biqn6E7rFpuy"}},{"cell_type":"code","source":["# pip install pandas scikit-learn nltk\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# 1. Load the Dataset\n","# Ensure 'IMDB Dataset.csv' is in your working directory\n","try:\n","    df = pd.read_csv('IMDB Dataset.csv')\n","    print(\"Dataset loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: 'IMDB Dataset.csv' not found. Please download it from Kaggle.\")\n","    exit()\n","\n","# 2. Pre-processing Function\n","def clean_text(text):\n","    # Remove HTML tags (e.g., <br />) common in this dataset\n","    text = re.sub(r'<.*?>', '', text)\n","    # Remove non-alphabetic characters (keep only letters)\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Convert to lowercase\n","    text = text.lower()\n","    return text\n","\n","print(\"Cleaning data... (this might take a moment)\")\n","df['cleaned_review'] = df['review'].apply(clean_text)\n","\n","# 3. Prepare Features (X) and Labels (y)\n","X = df['cleaned_review']\n","# Convert labels: 'positive' -> 1, 'negative' -> 0\n","y = df['sentiment'].map({'positive': 1, 'negative': 0})\n","\n","# 4. Split Data (80% Training, 20% Testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 5. Vectorization (TF-IDF)\n","# We limit to top 5000 words to keep the model fast\n","tfidf = TfidfVectorizer(max_features=5000)\n","\n","print(\"Vectorizing data...\")\n","X_train_vec = tfidf.fit_transform(X_train)\n","X_test_vec = tfidf.transform(X_test)\n","\n","# 6. Train Model (Logistic Regression)\n","print(\"Training model...\")\n","model = LogisticRegression()\n","model.fit(X_train_vec, y_train)\n","\n","# 7. Evaluate\n","predictions = model.predict(X_test_vec)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(\"\\n\" + \"=\"*30)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","print(\"=\"*30)\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(y_test, predictions, target_names=['Negative', 'Positive']))\n","\n","# 8. Test on New Custom Reviews\n","new_reviews = [\n","    \"The movie was fantastic! Great acting and plot.\",\n","    \"It was a complete waste of time. Boring and predictable.\"\n","]\n","new_vec = tfidf.transform(new_reviews)\n","preds = model.predict(new_vec)\n","\n","print(\"\\n--- Custom Tests ---\")\n","for review, pred in zip(new_reviews, preds):\n","    label = \"Positive\" if pred == 1 else \"Negative\"\n","    print(f\"Review: '{review}' -> Sentiment: {label}\")"],"metadata":{"id":"fJ5u1e9tE0RW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q11 - Build and train a text classifier for the given data (using textbob or\n","simpletransformers library)"],"metadata":{"id":"bwryVt-oFrzF"}},{"cell_type":"code","source":["# pip install textblob\n","from textblob import TextBlob\n","from textblob.classifiers import NaiveBayesClassifier\n","\n","# 1. Prepare Data\n","# TextBlob expects a list of tuples: [(text, label), ...]\n","train_data = [\n","    (\"I love this movie, it's amazing!\", 'pos'),\n","    (\"The plot was boring and slow.\", 'neg'),\n","    (\"What a fantastic performance by the actor.\", 'pos'),\n","    (\"I fell asleep halfway through.\", 'neg'),\n","    (\"The cinematography was beautiful.\", 'pos'),\n","    (\"This is a waste of time and money.\", 'neg')\n","]\n","\n","test_data = [\n","    (\"The movie was good.\", 'pos'),\n","    (\"I did not like the ending.\", 'neg')\n","]\n","\n","# 2. Train the Classifier\n","print(\"Training TextBlob Classifier...\")\n","cl = NaiveBayesClassifier(train_data)\n","\n","# 3. Evaluate\n","print(f\"Accuracy: {cl.accuracy(test_data) * 100:.2f}%\")\n","\n","# 4. Predict on new text\n","new_review = \"The director did a horrible job.\"\n","prob_dist = cl.prob_classify(new_review)\n","\n","print(f\"\\nReview: '{new_review}'\")\n","print(f\"Predicted: {prob_dist.max()}\")\n","print(f\"Confidence: {round(prob_dist.prob(prob_dist.max()), 2)}\")"],"metadata":{"id":"XrzSqjerE9iG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q12 - Generate text using a character-based RNN using an appropriate dataset. Given a\n","sequence of characters from a given data (eg \"Shakespear\"), train a model to predict\n","the next character in the sequence (\"e\")."],"metadata":{"id":"izQFB41pFu__"}},{"cell_type":"code","source":["# pip install tensorflow numpy requests\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import requests\n","\n","# --- 1. Load Data ---\n","# Download Shakespeare dataset (tiny shakespeare)\n","url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","text = requests.get(url).text\n","\n","print(f\"Corpus length: {len(text)} characters\")\n","\n","# Take a smaller slice for quick training demonstration (first 100k chars)\n","# In a real scenario, use the whole text.\n","text = text[:100000]\n","\n","# --- 2. Vectorize Text (Char to Int) ---\n","# Find all unique characters\n","vocab = sorted(set(text))\n","vocab_size = len(vocab)\n","print(f\"Unique characters: {vocab_size}\")\n","\n","# Create mapping from char to index and index to char\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","# Convert whole text to integers\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","# --- 3. Create Training Sequences ---\n","# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text) // (seq_length + 1)\n","\n","# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","# Create sequences of 101 characters (100 input, 1 target)\n","sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]       # \"Shakespear\"\n","    target_text = chunk[1:]       # \"hakespeare\" (Shifted by 1)\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n","\n","# Batch size and Buffer size for shuffling\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","# --- 4. Build the Model ---\n","vocab_size = len(vocab)\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.LSTM(rnn_units,\n","                            return_sequences=True,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","    ])\n","    return model\n","\n","model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n","\n","# --- 5. Train the Model ---\n","def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","# Directory to save checkpoints\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","print(\"Starting training... (This may take a few minutes)\")\n","# Train for 5 epochs for demonstration (increase to 30+ for good results)\n","history = model.fit(dataset, epochs=5, callbacks=[checkpoint_callback])\n","\n","# --- 6. Generate Text ---\n","def generate_text(model, start_string):\n","    # Rebuild model with batch_size=1 for prediction\n","    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","    model.build(tf.TensorShape([1, None]))\n","\n","    # Number of characters to generate\n","    num_generate = 500\n","\n","    # Converting our start string to numbers (vectorizing)\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    text_generated = []\n","\n","    # Low temperature results in more predictable text.\n","    # Higher temperature results in more surprising text.\n","    temperature = 1.0\n","\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # using a categorical distribution to predict the character returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # We pass the predicted character as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))\n","\n","print(\"\\n--- GENERATED TEXT ---\\n\")\n","print(generate_text(model, start_string=u\"ROMEO: \"))"],"metadata":{"id":"aa4A51G0FLB2"},"execution_count":null,"outputs":[]}]}